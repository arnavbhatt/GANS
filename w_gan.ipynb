{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "from tqdm import tqdm\n",
    "import nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the dataset\n",
    "class DataLoaderBuilder():\n",
    "    def __init__(self, root_path, image_size=(64, 64), batch_size=64, mean=0.5, std=0.5, shuffle=True):\n",
    "        self.root_path = root_path\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.shuffle = shuffle\n",
    "    \n",
    "    def get_dataloader(self):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(self.image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "        # Dataset\n",
    "        dataset = datasets.ImageFolder(root=self.root_path, transform=transform)\n",
    "        \n",
    "        # DataLoader\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=self.shuffle)\n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim, feature_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # Input is of shape N x (input_channels) x 64 x 64\n",
    "            nn.Conv2d(input_dim, feature_dim, kernel_size=4, stride=2, padding=1),  # 32x32\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(feature_dim, feature_dim * 2, kernel_size=4, stride=2, padding=1),  # 16x16\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(feature_dim * 2, feature_dim * 4, kernel_size=4, stride=2, padding=1),  # 8x8\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(feature_dim * 4, feature_dim * 8, kernel_size=4, stride=2, padding=1),  # 4x4\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(feature_dim * 8, 1, kernel_size=4, stride=1, padding=0),  # 1x1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, output_channels, feature_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # Input is of shape N x (latent_dim) x 1 x 1\n",
    "            nn.ConvTranspose2d(latent_dim, feature_dim * 16, kernel_size=4, stride=1, padding=0),  # 4x4\n",
    "            nn.BatchNorm2d(feature_dim * 16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(feature_dim * 16, feature_dim * 8, kernel_size=4, stride=2, padding=1),  # 8x8\n",
    "            nn.BatchNorm2d(feature_dim * 8),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(feature_dim * 8, feature_dim * 4, kernel_size=4, stride=2, padding=1),  # 16x16\n",
    "            nn.BatchNorm2d(feature_dim * 4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(feature_dim * 4, feature_dim * 2, kernel_size=4, stride=2, padding=1),  # 32x32\n",
    "            nn.BatchNorm2d(feature_dim * 2),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.ConvTranspose2d(feature_dim * 2, output_channels, kernel_size=4, stride=2, padding=1),  # 64 x 64\n",
    "            nn.Tanh()  # Output is of range [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset_path = \"../celeb_a_dataset\"\n",
    "dataloader = DataLoaderBuilder(root_path=dataset_path, image_size=(64, 64), batch_size=64).get_dataloader() \n",
    "\n",
    "latent_dim = 100\n",
    "output_channels = 3\n",
    "feature_dim = 64\n",
    "batch_size = 64\n",
    "weight_clipping = 0.01\n",
    "\n",
    "G = Generator(latent_dim, output_channels, feature_dim).to(device)\n",
    "C = Critic(output_channels, feature_dim).to(device)\n",
    "initialize_weights(G)\n",
    "initialize_weights(C)\n",
    "\n",
    "C_loss = []\n",
    "G_loss = []\n",
    "\n",
    "lr_C = 5e-5\n",
    "lr_G = 5e-5\n",
    "\n",
    "# Using RMSprop as its mentioned in the paper \n",
    "C_optimizer = optim.RMSprop(C.parameters(), lr=lr_C)\n",
    "G_optimizer = optim.RMSprop(G.parameters(), lr=lr_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 65\n",
    "n_critic = 5\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for i, (real_images, _) in enumerate(dataloader):\n",
    "        real_images = real_images.to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "\n",
    "        # Train Critic\n",
    "        for _ in range(n_critic):\n",
    "            latent_vector = torch.randn((batch_size, latent_dim, 1, 1)).to(device)\n",
    "            fake_images = G(latent_vector).detach()  # Detach to avoid generator gradients\n",
    "\n",
    "            critic_real = C(real_images).view(-1)\n",
    "            critic_fake = C(fake_images).view(-1)\n",
    "\n",
    "            loss_critic = torch.mean(critic_fake) - torch.mean(critic_real)\n",
    "\n",
    "            C_optimizer.zero_grad()\n",
    "            loss_critic.backward()\n",
    "            C_optimizer.step()\n",
    "\n",
    "            # Weight clipping\n",
    "            for param in C.parameters():\n",
    "                param.data.clamp_(-weight_clipping, weight_clipping)\n",
    "\n",
    "            C_loss.append(loss_critic.item())\n",
    "\n",
    "        # Train Generator\n",
    "        latent_vector = torch.randn((batch_size, latent_dim, 1, 1)).to(device)\n",
    "        fake_images = G(latent_vector)\n",
    "        critic_fake = C(fake_images).view(-1)\n",
    "        loss_G = -torch.mean(critic_fake)\n",
    "\n",
    "        G_optimizer.zero_grad()\n",
    "        loss_G.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        G_loss.append(loss_G.item())\n",
    "\n",
    "    if (epoch) % 5 == 0:\n",
    "        torch.save(G.state_dict(), f\"wgan_generator.pth\")\n",
    "        torch.save(C.state_dict(), f\"wgan_critic.pth\")\n",
    "        print(f\"Saved model weights at epoch {epoch}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate and visualize images\n",
    "def generate_images(generator, latent_dim, num_images, device, save_path=None):\n",
    "    # Set the generator to evaluation mode\n",
    "    generator.eval()\n",
    "\n",
    "    # Generate random noise vectors\n",
    "    noise = torch.randn((num_images, latent_dim, 1, 1), device=device)\n",
    "\n",
    "    # Generate fake images\n",
    "    with torch.no_grad():  # No need to calculate gradients\n",
    "        fake_images = generator(noise)\n",
    "\n",
    "    # Normalize the images to [0, 1] for visualization\n",
    "    fake_images = (fake_images + 1) / 2  # Since the generator outputs images in range [-1, 1]\n",
    "\n",
    "    # Create a grid of images\n",
    "    grid = vutils.make_grid(fake_images, nrow=8, normalize=True)\n",
    "\n",
    "    # Visualize the images\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu())  # Convert CHW to HWC for visualization\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Generated Images\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Example usage\n",
    "latent_dim = 100\n",
    "num_images = 64\n",
    "generate_images(G, latent_dim, num_images, device, save_path=\"generated_images.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
